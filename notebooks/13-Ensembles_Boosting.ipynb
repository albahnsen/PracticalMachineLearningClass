{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 - Ensemble Methods - Boosting\n",
    "\n",
    "\n",
    "by [Alejandro Correa Bahnsen](albahnsen.com/) and [Jesus Solano](https://github.com/jesugome)\n",
    "\n",
    "version 1.5, February 2019\n",
    "\n",
    "## Part of the class [Practical Machine Learning](https://github.com/albahnsen/PracticalMachineLearningClass)\n",
    "\n",
    "\n",
    "\n",
    "This notebook is licensed under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/deed.en_US). Special thanks goes to [Kevin Markham](https://github.com/justmarkham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we learning about ensembling?\n",
    "\n",
    "- Very popular method for improving the predictive performance of machine learning models\n",
    "- Provides a foundation for understanding more sophisticated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Combination of classifiers - Majority Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The most typical form of an ensemble is made by combining $T$ different base classifiers.\n",
    "  Each  base classifier $M(\\mathcal{S}_j)$ is trained by applying algorithm $M$ to a random subset \n",
    "  $\\mathcal{S}_j$ of the training set $\\mathcal{S}$.  \n",
    "  For simplicity we define $M_j \\equiv  M(\\mathcal{S}_j)$ for $j=1,\\dots,T$, and \n",
    "  $\\mathcal{M}=\\{M_j\\}_{j=1}^{T}$ a set of base classifiers.\n",
    "  Then, these models are combined using majority voting to create the ensemble $H$ as follows\n",
    "  $$\n",
    "    f_{mv}(\\mathcal{S},\\mathcal{M}) = max_{c \\in \\{0,1\\}} \\sum_{j=1}^T \n",
    "    \\mathbf{1}_c(M_j(\\mathcal{S})).\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and prepare the chrun data\n",
    "# Download the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/datasets/churn.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Create X and y\n",
    "\n",
    "# Select only the numeric features\n",
    "X = data.iloc[:, [1,2,6,7,8,9,10]].astype(np.float)\n",
    "# Convert bools to floats\n",
    "X = X.join((data.iloc[:, [4,5]] == 'no').astype(np.float))\n",
    "\n",
    "y = (data.iloc[:, -1] == 'True.').astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Area Code</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123.0</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114.0</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71.0</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113.0</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Account Length  Area Code  VMail Message  Day Mins  Day Calls  Day Charge  \\\n",
       "0           128.0      415.0           25.0     265.1      110.0       45.07   \n",
       "1           107.0      415.0           26.0     161.6      123.0       27.47   \n",
       "2           137.0      415.0            0.0     243.4      114.0       41.38   \n",
       "3            84.0      408.0            0.0     299.4       71.0       50.90   \n",
       "4            75.0      415.0            0.0     166.7      113.0       28.34   \n",
       "\n",
       "   Eve Mins  Int'l Plan  VMail Plan  \n",
       "0     197.4         1.0         0.0  \n",
       "1     195.5         1.0         0.0  \n",
       "2     121.2         1.0         1.0  \n",
       "3      61.9         0.0         1.0  \n",
       "4     148.3         0.0         1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2850</td>\n",
       "      <td>0.855086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>0.144914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  percentage\n",
       "0   2850    0.855086\n",
       "1    483    0.144914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts().to_frame('count').assign(percentage = lambda x: x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 100 decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "# create bootstrap samples (will be used to select rows from the DataFrame)\n",
    "samples = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(n_estimators)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "np.random.seed(123) \n",
    "seeds = np.random.randint(1, 10000, size=n_estimators)\n",
    "\n",
    "trees = {}\n",
    "for i in range(n_estimators):\n",
    "    trees[i] = DecisionTreeClassifier(max_features=\"sqrt\", max_depth=None, random_state=seeds[i])\n",
    "    trees[i].fit(X_train.iloc[samples[i]], y_train.iloc[samples[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   ...  90  91  92  93  94  95  96  \\\n",
       "438    0   0   0   0   0   0   0   0   0   0  ...   1   0   0   0   0   0   0   \n",
       "2674   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1345   0   0   0   1   0   0   0   0   0   1  ...   0   0   0   1   1   0   0   \n",
       "1957   0   0   0   0   0   0   0   0   0   1  ...   1   0   1   0   0   0   0   \n",
       "2148   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   1   0   \n",
       "\n",
       "      97  98  99  \n",
       "438    0   0   0  \n",
       "2674   0   0   0  \n",
       "1345   1   1   0  \n",
       "1957   0   1   0  \n",
       "2148   0   1   0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict \n",
    "y_pred_df = pd.DataFrame(index=X_test.index, columns=list(range(n_estimators)))\n",
    "for i in range(n_estimators):\n",
    "    y_pred_df.iloc[:, i] = trees[i].predict(X_test)\n",
    "\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438      2\n",
       "2674     5\n",
       "1345    35\n",
       "1957    17\n",
       "2148     3\n",
       "3106     4\n",
       "1786    22\n",
       "321      6\n",
       "3082    10\n",
       "2240     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.sum(axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5245901639344264"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_pred_df.sum(axis=1) >= (n_estimators / 2)).astype(np.int)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8945454545454545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, bootstrap=True,\n",
    "                        random_state=42, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.536, 0.8945454545454545)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Combination of classifiers - Weighted Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority voting approach gives the same weight to each classfier regardless of the performance of each one. Why not take into account the oob performance of each classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in the traditional approach, a \n",
    "similar comparison of the votes of the base classifiers is made, but giving a weight $\\alpha_j$ \n",
    "to each classifier $M_j$ during the voting phase\n",
    "$$\n",
    "  f_{wv}(\\mathcal{S},\\mathcal{M}, \\alpha)\n",
    "  =\\max_{c \\in \\{0,1\\}} \\sum_{j=1}^T \\alpha_j \\mathbf{1}_c(M_j(\\mathcal{S})),\n",
    "$$\n",
    "where $\\alpha=\\{\\alpha_j\\}_{j=1}^T$.\n",
    "The calculation of $\\alpha_j$ is related to the performance of each classifier $M_j$.\n",
    "It is usually defined as the normalized misclassification error   $\\epsilon$ of the base \n",
    "classifier $M_j$  in the out of bag set   $\\mathcal{S}_j^{oob}=\\mathcal{S}-\\mathcal{S}_j$\n",
    "\\begin{equation}\n",
    "  \\alpha_j=\\frac{1-\\epsilon(M_j(\\mathcal{S}_j^{oob}))}{\\sum_{j_1=1}^T \n",
    "  1-\\epsilon(M_{j_1}(\\mathcal{S}_{j_1}^{oob}))}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select each oob sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_oob = []\n",
    "# show the \"out-of-bag\" observations for each sample\n",
    "for sample in samples:\n",
    "    samples_oob.append(sorted(set(range(n_samples)) - set(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the oob error of each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(n_estimators)\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    y_pred_ = trees[i].predict(X_train.iloc[samples_oob[i]])\n",
    "    errors[i] = 1 - metrics.accuracy_score(y_train.iloc[samples_oob[i]], y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'OOB error of each tree')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.scatter(range(n_estimators), errors)\n",
    "plt.xlim([0, n_estimators])\n",
    "plt.title('OOB error of each tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = (1 - errors) / (1 - errors).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum_1 = ((y_pred_df) * alpha).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438     0.019994\n",
       "2674    0.050009\n",
       "1345    0.350206\n",
       "1957    0.170230\n",
       "2148    0.030047\n",
       "3106    0.040100\n",
       "1786    0.219790\n",
       "321     0.059708\n",
       "3082    0.100208\n",
       "2240    0.050143\n",
       "1910    0.180209\n",
       "2124    0.190141\n",
       "2351    0.049892\n",
       "1736    0.950014\n",
       "879     0.039378\n",
       "785     0.219648\n",
       "2684    0.010104\n",
       "787     0.700482\n",
       "170     0.220404\n",
       "1720    0.020166\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5267489711934156, 0.8954545454545455)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (weighted_sum_1 >= 0.5).astype(np.int)\n",
    "\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Weighted voting with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.536, 0.8945454545454545)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, bootstrap=True,\n",
    "                        random_state=42, n_jobs=-1, oob_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(clf.n_estimators)\n",
    "y_pred_all_ = np.zeros((X_test.shape[0], clf.n_estimators))\n",
    "\n",
    "for i in range(clf.n_estimators):\n",
    "    oob_sample = ~clf.estimators_samples_[i]\n",
    "    y_pred_ = clf.estimators_[i].predict(X_train.values[oob_sample])\n",
    "    errors[i] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "    y_pred_all_[:, i] = clf.estimators_[i].predict(X_test)\n",
    "    \n",
    "alpha = (1 - errors) / (1 - errors).sum()\n",
    "y_pred = (np.sum(y_pred_all_ * alpha, axis=1) >= 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5476190476190478, 0.8963636363636364)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Combination of classifiers - Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The staking method consists in combining the different base classifiers by learning a \n",
    "second level algorithm on top of them. In this framework, once the base \n",
    "classifiers are constructed using the training set  $\\mathcal{S}$, a new set is constructed \n",
    "where the output of the base classifiers  are now considered as the features while keeping the \n",
    "class labels.\n",
    "\n",
    "Even though there is no restriction on which algorithm can be used as a second level learner, \n",
    "it is common to use a linear model, such as \n",
    "$$\n",
    "  f_s(\\mathcal{S},\\mathcal{M},\\beta) =\n",
    "  g \\left( \\sum_{j=1}^T \\beta_j M_j(\\mathcal{S}) \\right),\n",
    "$$\n",
    "where $\\beta=\\{\\beta_j\\}_{j=1}^T$, and $g(\\cdot)$ is the sign function \n",
    "$g(z)=sign(z)$ in the case of a linear regression or the sigmoid function, defined \n",
    "as $g(z)=1/(1+e^{-z})$, in the case of a logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get a new training set consisting of the output of every classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = pd.DataFrame(index=X_train.index, columns=list(range(n_estimators)))\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    X_train_2[i] = trees[i].predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   ...  90  91  92  93  94  95  96  \\\n",
       "2360   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1412   0   0   1   0   0   0   0   0   0   0  ...   1   0   0   0   0   0   0   \n",
       "1404   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   1   0   0   \n",
       "626    1   1   0   1   1   1   1   1   1   1  ...   1   1   1   1   1   1   1   \n",
       "347    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "      97  98  99  \n",
       "2360   0   0   0  \n",
       "1412   0   0   0  \n",
       "1404   0   0   0  \n",
       "626    1   1   1  \n",
       "347    0   0   0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegressionCV(cv = 5 )\n",
    "lr.fit(X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10088464, 0.10421338, 0.09358074, 0.09660953, 0.09697781,\n",
       "        0.09904032, 0.11090891, 0.09651815, 0.0935383 , 0.09123672,\n",
       "        0.10193353, 0.09820731, 0.09391233, 0.09549362, 0.09152095,\n",
       "        0.09644657, 0.0896993 , 0.09192601, 0.09677729, 0.09012681,\n",
       "        0.09833482, 0.09514   , 0.10454839, 0.10039995, 0.09661211,\n",
       "        0.09730309, 0.10910644, 0.10590111, 0.10264443, 0.10275915,\n",
       "        0.10611423, 0.09792985, 0.1031922 , 0.09266752, 0.09697419,\n",
       "        0.09517301, 0.08893016, 0.0989854 , 0.09045676, 0.09011887,\n",
       "        0.09902678, 0.09876865, 0.10538597, 0.09571805, 0.09629428,\n",
       "        0.08972837, 0.09176166, 0.08990744, 0.10143139, 0.10831536,\n",
       "        0.10061968, 0.09763531, 0.08939132, 0.10073731, 0.10174935,\n",
       "        0.10527521, 0.0975041 , 0.0960247 , 0.0893913 , 0.10042684,\n",
       "        0.10342512, 0.10139005, 0.0902557 , 0.10340747, 0.09330728,\n",
       "        0.09808251, 0.10159091, 0.0931049 , 0.0952795 , 0.1103413 ,\n",
       "        0.09358935, 0.09856276, 0.10599656, 0.09591917, 0.09811424,\n",
       "        0.09995932, 0.10232043, 0.10072486, 0.1021542 , 0.11252795,\n",
       "        0.09948089, 0.11507633, 0.09797436, 0.10095548, 0.10140987,\n",
       "        0.1027939 , 0.09173845, 0.09905491, 0.10019419, 0.10150457,\n",
       "        0.09837983, 0.10311664, 0.09911577, 0.08938209, 0.09940932,\n",
       "        0.10271853, 0.09256684, 0.09530341, 0.10527235, 0.0985654 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(y_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5365853658536585, 0.8963636363636364)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5647058823529412, 0.899090909090909)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_all_ = np.zeros((X_test.shape[0], clf.n_estimators))\n",
    "X_train_3 = np.zeros((X_train.shape[0], clf.n_estimators))\n",
    "\n",
    "for i in range(clf.n_estimators):\n",
    "\n",
    "    X_train_3[:, i] = clf.estimators_[i].predict(X_train)\n",
    "    y_pred_all_[:, i] = clf.estimators_[i].predict(X_test)\n",
    "    \n",
    "lr = LogisticRegressionCV(cv=5)\n",
    "lr.fit(X_train_3, y_train)\n",
    "\n",
    "y_pred = lr.predict(y_pred_all_)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vs using only one dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.44378698224852065, 0.8290909090909091)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Boosting\n",
    "\n",
    "While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are typically weighted in some way that is usually related to the weak learners' accuracy. After a weak learner is added, the data is reweighted: examples that are misclassified gain weight and examples that are classified correctly lose weight (some boosting algorithms actually decrease the weight of repeatedly misclassified examples, e.g., boost by majority and BrownBoost). Thus, future weak learners focus more on the examples that previous weak learners misclassified. (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://vision.cs.chubu.ac.jp/wp/wp-content/uploads/2013/07/OurMethodv81.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://vision.cs.chubu.ac.jp/wp/wp-content/uploads/2013/07/OurMethodv81.png\", width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "AdaBoost (adaptive boosting) is an ensemble learning algorithm that can be used for classification or regression. Although AdaBoost is more resistant to overfitting than many machine learning algorithms, it is often sensitive to noisy data and outliers.\n",
    "\n",
    "AdaBoost is called adaptive because it uses multiple iterations to generate a single composite strong learner. AdaBoost creates the strong learner (a classifier that is well-correlated to the true classifier) by iteratively adding weak learners (a classifier that is only slightly correlated to the true classifier). During each round of training, a new weak learner is added to the ensemble and a weighting vector is adjusted to focus on examples that were misclassified in previous rounds. The result is a classifier that has higher accuracy than the weak learnersâ€™ classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "* Initialize all weights ($w_i$) to 1 / n_samples\n",
    "* Train a classifier $h_t$ using weights\n",
    "* Estimate training error $e_t$\n",
    "* set $alpha_t = log\\left(\\frac{1-e_t}{e_t}\\right)$\n",
    "* Update weights \n",
    "$$w_i^{t+1} = w_i^{t}e^{\\left(\\alpha_t \\mathbf{I}\\left(y_i \\ne h_t(x_t)\\right)\\right)}$$\n",
    "* Repeat while $e_t<0.5$ and $t<T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and prepare the chrun data\n",
    "# Download the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/datasets/churn.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Create X and y\n",
    "\n",
    "# Select only the numeric features\n",
    "X = data.iloc[:, [1,2,6,7,8,9,10]].astype(np.float)\n",
    "# Convert bools to floats\n",
    "X = X.join((data.iloc[:, [4,5]] == 'no').astype(np.float))\n",
    "\n",
    "y = (data.iloc[:, -1] == 'True.').astype(np.int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "n_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "weights = pd.DataFrame(index=X_train.index, columns=list(range(n_estimators)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "weights[t] = 1 / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "trees = []\n",
    "trees.append(DecisionTreeClassifier(max_depth=1))\n",
    "trees[t].fit(X_train, y_train, sample_weight=weights[t].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13613972234661886"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ = trees[t].predict(X_train)\n",
    "error = []\n",
    "error.append(1 - metrics.accuracy_score(y_pred_, y_train))\n",
    "error[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8477293114995077"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = []\n",
    "alpha.append(np.log((1 - error[t]) / error[t]))\n",
    "alpha[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[t + 1] = weights[t]\n",
    "filter_ = y_pred_ != y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.loc[filter_, t + 1] = weights.loc[filter_, t] * np.exp(alpha[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[t + 1] = weights[t + 1] / weights[t + 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 2 - n_estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, n_estimators):\n",
    "    trees.append(DecisionTreeClassifier(max_depth=1))\n",
    "    trees[t].fit(X_train, y_train, sample_weight=weights[t].values)\n",
    "    y_pred_ = trees[t].predict(X_train)\n",
    "    error.append(1 - metrics.accuracy_score(y_pred_, y_train))\n",
    "    alpha.append(np.log((1 - error[t]) / error[t]))\n",
    "    weights[t + 1] = weights[t]\n",
    "    filter_ = y_pred_ != y_train\n",
    "    weights.loc[filter_, t + 1] = weights.loc[filter_, t] * np.exp(alpha[t])\n",
    "    weights[t + 1] = weights[t + 1] / weights[t + 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13613972234661886,\n",
       " 0.15629198387819077,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classification\n",
    "\n",
    "Only classifiers when error < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_estimators = np.sum([x<0.5 for x in error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all = np.zeros((X_test.shape[0], new_n_estimators))\n",
    "for t in range(new_n_estimators):\n",
    "    y_pred_all[:, t] = trees[t].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (np.sum(y_pred_all * alpha[:new_n_estimators], axis=1) >= 1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5105105105105104, 0.8518181818181818)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29107981220657275, 0.8627272727272727)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5289256198347108, 0.8963636363636364)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
